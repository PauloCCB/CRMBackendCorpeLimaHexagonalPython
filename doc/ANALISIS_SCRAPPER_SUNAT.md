# An√°lisis del Endpoint `/obtener-ruc/{ruc}` y Estrategias de Optimizaci√≥n

## 1. ¬øQu√© es el WebDriver?

**WebDriver** es una interfaz de automatizaci√≥n de navegadores web que forma parte del proyecto Selenium. Permite controlar navegadores web de forma program√°tica como si fuera un usuario real.

### Caracter√≠sticas principales:

- **Automatizaci√≥n de navegadores**: Controla Chrome, Firefox, Safari, etc. program√°ticamente
- **Interacci√≥n con p√°ginas web**: Puede hacer clic, rellenar formularios, navegar entre p√°ginas
- **Extracci√≥n de datos**: Accede al DOM (Document Object Model) para extraer informaci√≥n
- **Ejecuci√≥n de JavaScript**: Puede ejecutar scripts dentro del navegador

### En tu c√≥digo:

El proyecto utiliza **Selenium WebDriver con Chrome** en modo headless (sin interfaz gr√°fica) para realizar web scraping en el sitio de SUNAT:

```python
# Ubicaci√≥n: app/adapters/outbound/external_services/sunat/sunat_scraper.py:68-106

def _create_driver(self):
    """Crea una nueva instancia de WebDriver"""
    options = self._get_chrome_options()

    # Configuraci√≥n headless para servidor
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    # Crea instancia de Chrome automatizado
    driver = webdriver.Chrome(service=service, options=options)
    return driver
```

### ¬øC√≥mo funciona el scraper?

1. **WebDriverManager Singleton**: Mantiene una √∫nica instancia del navegador Chrome activa por 12 horas
2. **Navegaci√≥n automatizada**: Accede a la p√°gina de SUNAT (`https://e-consultaruc.sunat.gob.pe/...`)
3. **Interacci√≥n**: Rellena el formulario con el RUC y hace clic en "Consultar"
4. **Extracci√≥n de datos**: Lee el DOM para obtener informaci√≥n del contribuyente
5. **Navegaci√≥n adicional**: Hace clics en botones para ver informaci√≥n de trabajadores y representantes legales
6. **Reintentos**: Implementa l√≥gica de 3 intentos con manejo de timeouts

---

## 2. Problema de Rendimiento y Estrategias de Optimizaci√≥n

### Diagn√≥stico del Problema Actual

El endpoint es **extremadamente lento** por las siguientes razones:

#### A. Naturaleza del WebDriver (10-30 segundos por consulta)
- Lanza un navegador Chrome completo (aunque sea headless)
- Carga HTML, CSS, JavaScript completo de SUNAT
- Espera a que elementos DOM est√©n disponibles con `WebDriverWait`
- Realiza m√∫ltiples navegaciones (p√°gina principal ‚Üí trabajadores ‚Üí representantes)

#### B. Limitaciones de Railway
- **Recursos limitados**: CPU y memoria restringidas en planes gratuitos/b√°sicos
- **Latencia de red**: El servidor debe conectarse a SUNAT desde su ubicaci√≥n
- **Cold starts**: Si el contenedor se suspende, el primer request es muy lento
- **ChromeDriver pesado**: Chrome consume mucha memoria (~200-300MB por instancia)

#### C. Arquitectura actual
- **Singleton de 12 horas**: Aunque reutiliza el driver, sigue siendo lento por navegaci√≥n
- **Operaciones s√≠ncronas**: Aunque usa `modo_rapido`, a√∫n necesita cargar p√°ginas completas
- **Sin cach√©**: Cada consulta golpea SUNAT directamente

---

## 3. Estrategias de Optimizaci√≥n

### üî• Estrategia 1: Implementar Sistema de Cach√© (IMPACTO ALTO - PRIORIDAD 1)

**Beneficio**: Reduce el 80-90% de consultas reales a SUNAT

#### Opci√≥n A: Cach√© en Memoria con TTL (R√°pida implementaci√≥n)

```python
# Implementar con Redis o cach√© en memoria
from cachetools import TTLCache
from datetime import timedelta

cache_ruc = TTLCache(maxsize=1000, ttl=86400)  # 24 horas

async def obtener_ruc(self, ruc: str) -> Dict:
    # Verificar cach√© primero
    if ruc in cache_ruc:
        print(f"‚úÖ RUC {ruc} encontrado en cach√©")
        return cache_ruc[ruc]

    # Si no est√° en cach√©, hacer scraping
    resultado = self.sunat_scraper.consultar_ruc(ruc)
    cache_ruc[ruc] = resultado
    return resultado
```

**Ventajas**:
- Respuesta instant√°nea para RUCs consultados recientemente
- Reduce carga en Railway y SUNAT
- F√°cil de implementar

**Desventajas**:
- Datos pueden quedar desactualizados (configurar TTL apropiado)
- Se pierde al reiniciar el servidor

#### Opci√≥n B: Cach√© Persistente con Redis (Recomendado)

```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

async def obtener_ruc(self, ruc: str) -> Dict:
    # Buscar en Redis
    cached = redis_client.get(f"ruc:{ruc}")
    if cached:
        return json.loads(cached)

    # Hacer scraping
    resultado = self.sunat_scraper.consultar_ruc(ruc)

    # Guardar en Redis con expiraci√≥n de 7 d√≠as
    redis_client.setex(
        f"ruc:{ruc}",
        timedelta(days=7),
        json.dumps(resultado)
    )
    return resultado
```

**Ventajas**:
- Persistencia entre reinicios
- Railway ofrece Redis como add-on
- Puede compartirse entre m√∫ltiples instancias

#### Opci√≥n C: Base de Datos PostgreSQL (M√°s robusto)

```python
# Guardar resultados en BD con timestamp
async def obtener_ruc(self, ruc: str) -> Dict:
    # Buscar en BD (√∫ltimos 7 d√≠as)
    cached = await db.query(
        "SELECT * FROM ruc_cache WHERE ruc = $1 AND updated_at > NOW() - INTERVAL '7 days'",
        ruc
    )
    if cached:
        return cached

    # Hacer scraping y guardar
    resultado = self.sunat_scraper.consultar_ruc(ruc)
    await db.execute(
        "INSERT INTO ruc_cache (ruc, data, updated_at) VALUES ($1, $2, NOW()) "
        "ON CONFLICT (ruc) DO UPDATE SET data = $2, updated_at = NOW()",
        ruc, json.dumps(resultado)
    )
    return resultado
```

**Ventajas**:
- Datos persistentes e hist√≥ricos
- Permite analytics y auditor√≠a
- Mayor control sobre invalidaci√≥n

---

### ‚ö° Estrategia 2: Cambiar de Selenium a Requests + BeautifulSoup (IMPACTO MEDIO-ALTO)

**Beneficio**: 5-10x m√°s r√°pido, consume 90% menos recursos

El sitio de SUNAT no requiere JavaScript para consultas b√°sicas, por lo que puedes usar HTTP directo:

```python
import requests
from bs4 import BeautifulSoup

def consultar_ruc_rapido(ruc: str) -> Dict:
    # Hacer request HTTP directo
    url = "https://e-consultaruc.sunat.gob.pe/cl-ti-itmrconsruc/jcrS00Alias"

    payload = {
        'accion': 'consPorRuc',
        'nroRuc': ruc
    }

    response = requests.post(url, data=payload, timeout=10)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extraer datos del HTML
    razon_social = soup.find('h4', string=lambda t: 'N√∫mero de RUC:' in t)
    # ... resto de extracci√≥n

    return datos
```

**Ventajas**:
- **10-100x m√°s r√°pido** que Selenium
- Consume solo ~10-20MB de RAM vs 200-300MB de Chrome
- M√°s estable y menos propenso a errores
- Railway puede manejar muchas m√°s solicitudes concurrentes

**Desventajas**:
- Requiere reescribir la l√≥gica de extracci√≥n
- Si SUNAT cambia la estructura HTML, hay que actualizar
- Puede requerir manejar headers/cookies para evitar bloqueos

**Recomendaci√≥n**: Este cambio tiene el **mayor impacto** en rendimiento. Comb√≠nalo con cach√© para resultados √≥ptimos.

---

### üöÄ Estrategia 3: Implementar Queue + Worker Background (IMPACTO MEDIO)

Si las consultas deben ser s√≠ncronas pero puedes tolerar cierta latencia, usa un sistema de colas:

```python
# FastAPI endpoint
@router.get("/obtener-ruc/{ruc}")
async def obtener_ruc(ruc: str):
    # Verificar cach√©
    cached = await cache.get(ruc)
    if cached:
        return cached

    # Agregar a cola de procesamiento
    job_id = await queue.enqueue('scrape_ruc', ruc)

    return {
        "status": "processing",
        "job_id": job_id,
        "check_url": f"/obtener-ruc/status/{job_id}"
    }

# Worker en background procesa la cola
# El cliente hace polling al endpoint de status
```

**Ventajas**:
- API responde inmediatamente
- Workers pueden escalar independientemente
- Evita timeouts de Railway

**Desventajas**:
- Requiere cambios en el frontend
- M√°s complejidad arquitect√≥nica

---

### üîß Estrategia 4: Optimizaciones de Railway y Deployment

#### A. Aumentar recursos del plan
- **Railway Pro**: M√°s CPU/RAM para manejar Chrome
- **Dedicated instances**: Evita cold starts

#### B. Configurar timeout apropiados
```python
# Aumentar timeouts en Railway
# railway.json
{
  "build": {
    "builder": "DOCKERFILE"
  },
  "deploy": {
    "healthcheckPath": "/health",
    "healthcheckTimeout": 300,
    "restartPolicyType": "ON_FAILURE"
  }
}
```

#### C. Pre-calentar el WebDriver
```python
# Al iniciar la app, crear el driver inmediatamente
@app.on_event("startup")
async def startup_event():
    scraper = SunatScraper()
    scraper.get_driver()  # Inicializa Chrome
    print("‚úÖ WebDriver pre-calentado")
```

#### D. Usar imagen Docker optimizada
```dockerfile
# Usar imagen m√°s ligera con Chrome pre-instalado
FROM zenika/alpine-chrome:latest
# O usar Playwright que es m√°s eficiente que Selenium
```

---

### üìä Estrategia 5: API Oficial de SUNAT (IMPACTO ALTO - Largo Plazo)

SUNAT ofrece APIs oficiales para consultas:

- **API REST de SUNAT**: Requiere registro y autenticaci√≥n
- **Web Service SOAP**: M√°s complejo pero oficial
- **Servicio de Padrones**: Para consultas masivas

**Ventajas**:
- Datos oficiales y actualizados
- Sin riesgo de bloqueo
- Mucho m√°s r√°pido y estable

**Desventajas**:
- Requiere tr√°mites y permisos
- Posiblemente tenga costos
- Tiempo de implementaci√≥n

---

## 4. Plan de Acci√≥n Recomendado (Priorizado)

### Fase 1: Quick Wins (1-3 d√≠as) üéØ
1. **Implementar cach√© con Redis** (o TTLCache si Redis no est√° disponible)
   - TTL: 7 d√≠as para datos b√°sicos
   - TTL: 1 d√≠a para trabajadores/representantes
   - Reducci√≥n esperada: **80-90% de consultas a SUNAT**

2. **Pre-calentar WebDriver al startup**
   - Elimina cold start del primer request

3. **Ajustar timeouts y reintentos**
   - Reducir de 3 a 2 reintentos
   - Timeouts m√°s agresivos en Railway

### Fase 2: Optimizaci√≥n Media (1 semana) ‚ö°
4. **Migrar de Selenium a Requests + BeautifulSoup**
   - Reescribir `sunat_scraper.py` con requests
   - Testing exhaustivo
   - **Impacto esperado: 5-10x m√°s r√°pido**

5. **Implementar monitoreo**
   - Logs de tiempo de respuesta
   - M√©tricas de cache hit rate
   - Alertas de timeouts

### Fase 3: Arquitectura Robusta (2-3 semanas) üèóÔ∏è
6. **Implementar sistema de colas (opcional)**
   - Celery + Redis o Bull Queue
   - Worker separado para scraping
   - API as√≠ncrona con webhooks

7. **Evaluar Railway Pro**
   - M√°s recursos si el volumen justifica
   - O considerar alternativas (Render, Fly.io)

### Fase 4: Soluci√≥n Definitiva (Largo plazo) üéì
8. **Migrar a API oficial de SUNAT**
   - Investigar requisitos
   - Realizar tr√°mites necesarios
   - Implementaci√≥n gradual

---

## 5. Estimaci√≥n de Mejoras

| Estrategia | Tiempo de Implementaci√≥n | Reducci√≥n de Latencia | Dificultad |
|-----------|-------------------------|----------------------|-----------|
| Cach√© Redis | 1-2 d√≠as | 90% (para hits) | Baja |
| Requests vs Selenium | 3-5 d√≠as | 80-90% | Media |
| Queue + Workers | 1-2 semanas | 50% (percibida) | Media-Alta |
| API Oficial SUNAT | 1-2 meses | 95% | Alta |
| Railway Pro | Inmediato | 30-50% | Baja |

---

## 6. C√≥digo de Ejemplo: Cach√© con Redis

```python
# app/core/use_cases/integracion_sunat/integracion_sunat_uc.py
import redis
import json
from datetime import timedelta

class IntegracionSunatUC:
    def __init__(self, sunat_scraper: SunatScraper):
        self.sunat_scraper = sunat_scraper
        # Conectar a Redis (Railway lo proporciona como add-on)
        self.redis_client = redis.from_url(
            os.getenv('REDIS_URL', 'redis://localhost:6379'),
            decode_responses=True
        )
        self.cache_ttl = 604800  # 7 d√≠as en segundos

    async def obtener_ruc(self, ruc: str, max_intentos: int = 3) -> Dict:
        # Validar formato
        if not self._validar_ruc(ruc):
            return {...}

        # 1. BUSCAR EN CACH√â PRIMERO
        try:
            cache_key = f"sunat:ruc:{ruc}"
            cached_data = self.redis_client.get(cache_key)

            if cached_data:
                print(f"‚úÖ Cache HIT para RUC {ruc}")
                return json.loads(cached_data)
            else:
                print(f"‚ùå Cache MISS para RUC {ruc}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error accediendo a cach√©: {e}")

        # 2. SI NO EST√Å EN CACH√â, HACER SCRAPING
        ultimo_error = None
        for intento in range(1, max_intentos + 1):
            try:
                resultado = self.sunat_scraper.consultar_ruc(ruc, modo_rapido=True)

                # Verificar si hubo error
                if "error" in resultado:
                    # ... manejo de errores
                    continue

                # 3. GUARDAR EN CACH√â SI LA CONSULTA FUE EXITOSA
                try:
                    self.redis_client.setex(
                        cache_key,
                        self.cache_ttl,
                        json.dumps(resultado)
                    )
                    print(f"üíæ RUC {ruc} guardado en cach√© por {self.cache_ttl} segundos")
                except Exception as e:
                    print(f"‚ö†Ô∏è Error guardando en cach√©: {e}")

                return resultado

            except Exception as e:
                # ... manejo de errores
                pass

        return {...}  # Error
```

---

## 7. Conclusiones

### Problema principal
El scraping con Selenium WebDriver es **inherentemente lento** (10-30 segundos por consulta) y consume muchos recursos en Railway.

### Soluci√≥n recomendada (combinaci√≥n)
1. **Cach√© agresivo** con Redis ‚Üí Elimina 80-90% de consultas lentas
2. **Migrar a requests/BeautifulSoup** ‚Üí 5-10x m√°s r√°pido que Selenium
3. **Optimizar Railway** ‚Üí M√°s recursos o configuraci√≥n mejorada

### Resultado esperado
- **Tiempo con cach√© (hit)**: < 100ms (reducci√≥n del 99%)
- **Tiempo sin cach√© (miss)**: 2-5 segundos con requests (vs 10-30s actual)
- **Capacidad**: 10-20x m√°s solicitudes concurrentes

### Pr√≥ximos pasos
1. Implementar Redis en Railway
2. A√±adir capa de cach√© al use case
3. Evaluar migraci√≥n a requests
4. Monitorear m√©tricas de rendimiento
